# AI Safety Draft Released

[Blog Index](../index.md)

AI action summit released International [AI safety Report](https://www.gov.uk/government/publications/international-ai-safety-report-2025) 
in January before Paris AI action Summit. The report is a columination of Bletchley Declaration which called for international 
cooperation for controlled design, development and deployment of AI preserving human safety. The Bletchley Declaration raises concerns about
AI's use in terrorism, criminal activities, and warfare.The report drafting committee was headed by Prof. Yoshua Bengio of Université de
Montréal/Qubec AI institute MILA. The committee had representations from 30 countries including US, 
China, EU and OECD nations, India, UK, Australia, and a few African nations. Though China is represented, the absence of Russia and 
poor symbolic reprentation from Middle East nations is a bit baffling. 

Two recent events suggests perhaps the report came a bit too early which may have not taken full potentials of unleasing capabilities of 
AI into safety consideration.
- The first one is announcement of [DeepSeek](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf) from a Chinese company
- The US president Donald Trump announced a massive funding of 500 billion USD for AI Research and Product develements. 

DeepSeek adopts multi-headed Latent Attention Model (MAL) using Mixture of Experts (MoE) with 671B parameters with 37B activated tokens. 
In pre-training phase it uses 14.8 trillion diverse high quality tokens followed by supervised fine-tunning and reinforced learning stages. 
In post-training phase it distills knowlegde by innovative reasoning on long chain of thoughts (CoT). From deployment angle, the most
important aspects of DeepSeek are:
- Low cost of development
- Low cost of deployment
  
The estimated cost of DeepSeek's development is 500 Million USD as against 340 Billion USD of OpenAI. At low-end, DeepSeek R1 can
run locally on a windows, macOS or linux dual chore device with 8GB RAM having 1GB free disk space. It is an effective democratization of
AI use for everyday use. With the power of AI in hand of ordinary person, the scope of use can be as imaginative as one can guess.
Donald Trump announced 500 billion USD funding for AI initiative in whitehous in presence of OpenAI CEO Sam Altman, Softbank CEO
Masayoshi Son and Oracle Chairman Larry Ellison. Interestingly, Elon Musk was not only excluded from the US government supported
AI iniatives though he was one of the six founders of OpenAI. OpenAI is not open anymore. It is a subcriber-based paid service.
With DeepSeek announcement,I guess US government initiative could do much more than perhaps Sam Altman group has initially
planned for. And Trump being most transactional POTUS may perhaps reduce allocation to 5 Billion Dollars and could eject the
OpenAI Soft Bank combination from US government's AI initiative. It will still increases the scope and the levels of deployments
of AI products in military and defense establishment million times more than what anyone may speculate. Lethal capacity of terrorism 
strike also increases proportionally. Under the given scenario the question of AI Safety becomes much more relevant than even 
what was anticipated when the Bletchley Declaration.  

The key findings of AI safety report is mainly focused on risks from the prospective of an invidual human being such as
- Harming individuals through fake content
- Manipulation of Public opinion
- Cyber offense
- Biological and chemical attacks

All of the above mentioned risks are already happening. So, the report only emphasizes the need of safety rules and laws
as deterrents against engaging in such acts. However, the acts of deliberate disinformation, manipulation public opinion or the
act of cyber offenses cannot be prevented unless a close world wide cooperation is guaranteed. Unfortunately, even recognized
world adjudication bodies like WHO, ICJ, Security Council or UN could only watch from a distance when:
- Wuhan lab's Covid virus ravaged the world during 2019-2020
- Russia Ukraine war broke
- Large scale Hamas led terrorist attacks occurred on Israel and subsequent middle east war.
- Deepstate actors are operating with impunity against legitmeate people mandated governments such as Bangladesh

If world bodies are powerless or can be manipulated by powerful nations the smaller less capable countries have no 
virtually zero chance of surrvival, then an individual will remain defenseless and safety recommendations will only
be seen as mere lip services or bunch of empty words. 

The AI safety report also deals with systemic risk due to large scale deployment of AI models focusing on following 
few societal aspects:
- Disruption in labour market leading to massive job loss with deployment of highly automated AI controlled equipments
- Disruption in R&D ecosystem of collaboration and dependencies across globe.
- Market concentration and single point failure
- Risk to the environment
- Risk to privacy
- Risk to the copyright infringment

The disruption in labour market is already happening job loss is a real thing. AI tools can perform better than an 
average person. However, there may be many unforeseen eventualities. For example, AI generated codes may let us develop 
and deploy a software very quickly. However, I believe engineers may find it difficult to maintain these AI generated code. 
If one does not understand the code or have not developed, then maintaining it is not easy. Training AI generators 
to fully understand legacy issues may not be a cake walk. Collaboration is a fundamental driving force in R&D ecosystem.
It inherently balances the knowledge sharing across the globe. Countries with large infrastructure support and 
proporationately more resources can catch up quickly on AI research and development. Therefore, the AI sumit action
group on safety feared that knowledge gap may widen considerably in R&D collaboration ecosystem. The other risk types
are only manifestation of problem with old world order which reappeared. In conclusion, all the systemic risks 
mentioned in the AI safety report arise basically due to AI enabled tools being under the controlled by a few large 
tech companies. These appear to be a rehash of the reigime of monopolistic trade related activities as well known but
present in a different format with enlarged scopes.

The release of DeepSeek has created an altogether different situcation. It has led to panic mode in big tech companies
that has invested heavily in AI. Nvidia shares lost 279 billion USD in a single day reacting to DeepSeek shock. As 
someone commented "you not need to drive a Tesla to pick up a pint of milk from a neighborhood store." Joke aside,
I think the scope of personal safety will now be less of a concern as individual or a small group of individual with very 
little resources can cause severe damages and pose much aggravated risks. Covert or overt political activities coul
bring in a whole new dimension of demagoguery. The world will have to embrace a hightened level of instability. 
Some mey view it as the God's way of restoring balance between individuals and organized systems like state and big 
enterprises. However the bottom line is that the AI safety issues require a whole new reevaluation than the report
prepared by Yoshua BengiO and his team of International experts.
