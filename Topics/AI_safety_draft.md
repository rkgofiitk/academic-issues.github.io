# AI Action Summit Releases AI Safety Report

[Blog Index](../index.md)

AI Action Summit released the International [AI safety Report](https://www.gov.uk/government/publications/international-ai-safety-report-2025) 
in January before the scheduled Paris Summit. The report is a culmination of the Bletchley Declaration, which called for 
international cooperation for the controlled design, development, and deployment of AI to preserve human privacy and safety. 
The Bletchley Declaration raised concerns about AI's use in terrorism, criminal activities, and warfare. Prof. Yoshua Bengio 
of Université de Montréal/Quebec AI Institute MILA headed the report drafting committee. The committee had representations from 30 
countries, including the US, China, EU and OECD nations, India, the UK, Australia, and a few African nations. Though China
is represented, the absence of Russia and only a symbolic representation from Middle-East nations baffles. 

The two key findings of the report from the point of view of safety are:
- Harms from general-purpose AI are well-established
- Additional unforeseen risks will emerge gradually as AI tools get deployed

One disturbing feature of individual harming identified in the report is the capability of the scams, non-consensual imagery,
and child sexual abuse material (CSAM) outputs from AI-enabled hacking. It cannot be blocked through legislation or 
legal enforcement. All three evils are recognized as known old societal evils. These existed without AI, too. AI-enabled hacking
has only made it easier. The second key finding amplifies that the extensive deployment of AI-enabled equipment
will disrupt the labor market. The effects are already felt in the manufacturing, construction, supply chain, and logistic movement sectors. However, the report mentions potential societal-scale risks in scientific reasoning tests and programming.  
Biological attacks are listed as an area of significant concern. 

Two recent events suggest the report came too early and prematurely. It may not have taken the full potential 
of unleashing the capabilities of AI to invade unforeseen safety considerations due to intensive ongoing research. 
- The first one is the announcement of [DeepSeek](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf) from a Chinese company
- US President Donald Trump announced a massive funding of 500 billion USD for AI Research and Product development. 

DeepSeek adopts a multi-headed Latent Attention Model (MAL) using a Mixture of Experts (MoE) with 671B parameters and 37B 
activated tokens. The pre-training phase uses 14.8 trillion diverse, high-quality tokens followed by supervised 
fine-tuning and reinforced learning stages. In the post-training phase, knowledge is distilled through innovative reasoning 
on a long chain of thoughts (CoT). From a deployment angle, the most important aspects of DeepSeek are:
- Low cost of development
- Low cost of deployment
  
The estimated cost of DeepSeek's development is 500-700 million USD, as opposed to the 340 billion USD of OpenAI. At the low
end, DeepSeek R1 can run locally on a Windows, MacOS, or Linux dual-chord personal device with 8GB RAM and 1GB free disk space. 
It is an effective democratization of AI's use barring the usual Chinese propaganda material. With the power of AI in the
hands of ordinary people, the scope of use can be as imaginative as one can guess. Donald Trump announced 500 billion USD for 
the AI initiative in the Whitehouse in the presence of OpenAI CEO Sam Altman, Softbank CEO Masayoshi Son, and Oracle Chairman 
Larry Ellison. Interestingly, Elon Musk was excluded from the US government-supported AI initiatives, though he was one of the
six pioneer founders of OpenAI. OpenAI is not open anymore. It is a subscriber-based paid service. With the DeepSeek 
announcement, the US government initiative could do much more than Sam Altman group initially planned. Trump, being the 
most transactional POTUS, may reduce the allocation to 5 Billion Dollars and could even eject the OpenAI-Soft Bank-Orcale
combination from the US government's AI initiative. It will still increase the scope and the levels of deployments of AI products
in defense establishments or state-crafts a million times more than anyone can speculate. The lethal capacity of terrorist
strikes also increases exponentially. Under the given scenario, the question of AI Safety becomes much more relevant than
anticipated at the time of the Bletchley Declaration.  

The AI safety report extensively focused on risks from the perspective of an individual human being, such as
- Harming individuals through fake content
- Manipulation of Public opinion
- Cyber offense
- Biological and chemical attacks

All the risks mentioned above are pre-AI problems the world has learned to deal with. So, the report
only emphasizes the need for more stringent safety rules and laws as deterrents against engaging in such acts. However, 
deliberate disinformation, manipulation of public opinion, or cyber offenses cannot be prevented unless close worldwide
cooperation is guaranteed. Unfortunately, even recognized world adjudication bodies like WHO, ICJ, Security Council, or UN are
found dysfunctional and watch from a distance. Take, for instance, the following cases,
- Wuhan lab's Covid virus ravaging the world during 2019-2020
- The Russia-Ukraine war
- Hamas terrorists unleashed large-scale attacks on innocent civilians in Israel that led to subsequent Middle East war.
- Deep state actors are operating with impunity against legitimate people-mandated governments in Bangladesh

The world bodies are either powerless or can be manipulated by powerful nations. Smaller nations with little or
no resources have virtually zero chance of survival. Parts of Africa,  Latin America, and South Asian nations might not have
survived without Indian COVID-19 vaccines. Under the given background, an individual will remain defenseless, and safety 
recommendations should only be viewed as mere lip service or a bunch of empty words. 

Of course, the AI safety report deals with systemic risks due to the large-scale deployment of AI models focusing on
the following few societal aspects, such as:
- Disruption in the labor market
- Disruption in the R&D ecosystem of collaboration and dependencies 
- Market concentration and single-point failure
- Risk to the environment
- Risk to privacy
- Risk to copyright infringement

The point is that AI tools can perform better than an average person in control and reasoning. Therefore, we find
large-scale deployment of AI-controlled equipment in construction and manufacturing. It has led to the elimination of
labor-intensive components everywhere. However, there may be many unforeseen risks, especially in soft-core
jobs. For example, AI-generated codes may allow us to develop and deploy software quickly. However, engineers may 
find it challenging to maintain AI-generated codes and systems. If one does not understand the code or has not developed it, 
then maintaining it is not easy. Training AI generators to fully understand legacy issues may not be a cakewalk.
Collaboration is a fundamental driving force in the R&D ecosystem. It inherently balances knowledge sharing across
the globe. Countries with extensive infrastructure support and proportionately more resources can catch up quickly
on AI research and development. Therefore, the AI action group on safety feared that the knowledge gap may widen 
considerably in the R&D collaboration ecosystem. The other risk types are manifestations of problems with the 
old world order, which have only reappeared. In conclusion, all the systemic risks 
mentioned in the AI safety report arise basically due to AI-enabled tools being under the control of a few large 
tech companies. These are a rehash of the old regime of monopolistic trade practices before the principles of WTO  
came into force. However, WTO agreements did not deter Trump from announcing trade tariffs on Mexico, Canada, and China.
The tariff war on Mexico is particularly interesting regarding the effectiveness of WTO. China largely avoided
The US imposed tariffs by onshoring manufacturing entities in Mexico. 

The release of DeepSeek has created an altogether different situation. It has led to panic in big tech companies
that have invested heavily in AI. In a single day, reacting to DeepSeek's shock, Nvidia shares lost 279 billion USD. As 
someone commented, "You do not need to drive a Tesla to pick up a pint of milk from a neighborhood store." Joke aside,
the scope of personal safety will now be less of a concern. An individual or a small group with  
few resources can inflict severe damage and pose much more aggravated risks to even large and powerful nations. Covert
or overt political activities could bring in a new dimension of demagoguery. It is happening already.
The entire world will have to embrace a heightened level of instability. Some may view it as God's way of restoring the
balance between individuals and organized systems like the state and big enterprises. However, the bottom line is that 
the AI safety issues require a whole new reevaluation compared to the report prepared by Yoshua BengiO and his team of 
international experts. However, the bottom line is that the safety issues of AI require reevaluation.

[Back to Index](../index.md)
