# AI Safety Draft Released

[Blog Index](../index.md)

AI Action Summit released the International [AI safety Report](https://www.gov.uk/government/publications/international-ai-safety-report-2025) 
in January before the Paris AI Action Summit. The report is a culmination of the Bletchley Declaration, which called for international 
cooperation for the controlled design, development, and deployment of AI to preserve human safety. The Bletchley Declaration raises concerns about
AI's use in terrorism, criminal activities, and warfare. The report drafting committee was headed by Prof. Yoshua Bengio of Université de
Montréal/Qubec AI institute MILA. The committee had representations from 30 countries, including the US, 
China, EU and OECD nations, India, the UK, Australia, and a few African nations. Though China is represented, the absence of Russia and 
poor symbolic representation from Middle-East nations baffles. 

Two recent events suggest perhaps the report came a bit too early, which may not have taken the full potential of unleashing the capabilities of 
AI into safety consideration.
- The first one is the announcement of [DeepSeek](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf) from a Chinese company
- US President Donald Trump announced a massive funding of 500 billion USD for AI Research and Product development. 

DeepSeek adopts a multi-headed Latent Attention Model (MAL) using a Mixture of Experts (MoE) with 671B parameters and 37B activated tokens. 
The pre-training phase uses 14.8 trillion diverse, high-quality tokens followed by supervised fine-tuning and reinforced learning stages. 
In the post-training phase, knowledge is distilled through innovative reasoning on a long chain of thoughts (CoT). From a deployment angle, the most
important aspects of DeepSeek are:
- Low cost of development
- Low cost of deployment
  
The estimated cost of DeepSeek's development is 500 million USD, as opposed to the 340 billion USD of OpenAI. At the low end, DeepSeek R1 can
run locally on a Windows, MacOS, or Linux dual-chord device with 8GB RAM and 1GB free disk space. It is an effective democratization of
AI for everyday use. With the power of AI in the hands of ordinary people, the scope of use can be as imaginative as one can guess.
Donald Trump announced 500 billion USD for the AI initiative in the Whitehouse in the presence of OpenAI CEO Sam Altman, Softbank CEO.
Masayoshi Son and Oracle Chairman Larry Ellison. Interestingly, Elon Musk was not only excluded from the US government-supported
AI initiatives, though he was one of the six founders of OpenAI. OpenAI is not open anymore. It is a subscriber-based paid service.
With the DeepSeek announcement, the US government initiative could do much more than the Sam Altman group initially planned. Trump, 
being the most transactional POTUS, may perhaps reduce allocation to 5 Billion Dollars and could eject the
OpenAI Soft Bank combination from the US government's AI initiative. It will still increase the scope and the levels of deployments
of AI products in military and defense establishments a million times more than anyone may speculate. The lethal capacity of terrorist 
strikes also increases proportionally. Under the given scenario, the question of AI Safety becomes much more relevant than  
what was anticipated when the Bletchley Declaration.  

The key findings of the AI safety report are mainly focused on risks from the perspective of an individual human being, such as
- Harming individuals through fake content
- Manipulation of Public opinion
- Cyber offense
- Biological and chemical attacks

All of the risks mentioned above are already happening. So, the report only emphasizes the need for safety rules and laws
as deterrents against engaging in such acts. However, deliberate disinformation, manipulation of public opinion, or cyber offenses 
cannot be prevented unless close worldwide cooperation is guaranteed. Unfortunately, even recognized
world adjudication bodies like WHO, ICJ, Security Council, or UN could only watch from a distance when:
- Wuhan lab's Covid virus ravaged the world during 2019-2020
- The Russia-Ukraine war broke
- Large-scale Hamas-led terrorist attacks occurred in Israel and subsequent Middle East war.
- Deep state actors are operating with impunity against legitimate people-mandated governments such as Bangladesh

If world bodies are powerless or can be manipulated by powerful nations, the smaller, less capable countries have no 
virtually zero chance of survival, then an individual will remain defenseless, and safety recommendations will only
be seen as mere lip service or a bunch of empty words. 

The AI safety report also deals with systemic risk due to the large-scale deployment of AI models focusing on the following 
few societal aspects:
- Disruption in the labor market leading to massive job loss with the deployment of highly automated AI-controlled equipment
- Disruption in the R&D ecosystem of collaboration and dependencies across the globe.
- Market concentration and single-point failure
- Risk to the environment
- Risk to privacy
- Risk to copyright infringement

The disruption in the labor market is already happening. Job loss is a real thing. AI tools can perform better than an 
average person. However, there may be many unforeseen eventualities. For example, AI-generated codes may allow us to develop 
and deploy software quickly. However, engineers may find it challenging to maintain this AI-generated code. 
If one does not understand the code or has not developed it, then maintaining it is not easy. Training AI generators 
to fully understand legacy issues may not be a cakewalk. Collaboration is a fundamental driving force in the R&D ecosystem.
It inherently balances knowledge sharing across the globe. Countries with extensive infrastructure support and 
proportionately more resources can catch up quickly on AI research and development. Therefore, the AI action
group on safety feared that the knowledge gap may widen considerably in the R&D collaboration ecosystem. The other risk types
are only manifestations of problems with the old world order, which reappeared. In conclusion, all the systemic risks 
mentioned in the AI safety report arise basically due to AI-enabled tools being under the control of a few large 
tech companies. These are a rehash of the old regime of monopolistic trade practices before the principles of WTO  came into force.  

The release of DeepSeek has created an altogether different situation. It has led to panic mode in big tech companies
that have invested heavily in AI. In a single day, Nvidia shares lost 279 billion USD, reacting to DeepSeek's shock. As 
someone commented, "You do not need to drive a Tesla to pick up a pint of milk from a neighborhood store." Joke aside,
The scope of personal safety will now be less of a concern as an individual or a small group with  
few resources can inflict severe damage and pose much more aggravated risks. Covert or overt political activities could
bring a new dimension of demagoguery. The world will have to embrace a heightened level of instability. 
Some may view it as God's way of restoring the balance between individuals and organized systems like the state and big 
enterprises. However, the bottom line is that the AI safety issues require a whole new reevaluation compared to the report
prepared by Yoshua BengiO and his team of international experts.big enterprises. However the bottom line is that the AI 
safety issues require a whole new reevaluation than the report prepared by Yoshua BengiO and his team of International experts.

[Back to Index](../index.md)
